{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6a7e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c7d2cb-7809-483f-9c91-87ef98a3c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# extraction\n",
    "import time\n",
    "import pyautogui\n",
    "import keyboard\n",
    "\n",
    "# video\n",
    "import cv2\n",
    "from keras.models import save_model\n",
    "import face_recognition\n",
    "import tensorflow as tf # 딥러닝 라이브러리\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "\n",
    "# voice\n",
    "from moviepy.editor import VideoFileClip\n",
    "import dlib\n",
    "from speechbrain.pretrained.interfaces import foreign_class\n",
    "\n",
    "# text\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2aa35f-2311-45c6-97c2-c499e0e35dd4",
   "metadata": {},
   "source": [
    "## 전달할 값 : face_emotion, voice_emotion, stance, interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939bb8f2-4406-42d4-bee7-32fa09704bfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MOV_to_MP4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# start_recording()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# stop_recording()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     mov_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpowell video.mov\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# File Name 불러오는 방법 수정해야 함\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mMOV_to_MP4\u001b[49m(mov_file_path)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# face\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     network \u001b[38;5;241m=\u001b[39m create_cnn_model()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MOV_to_MP4' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # start_recording()\n",
    "    # stop_recording()\n",
    "    mov_file_path = 'powell video.mov' # File Name 불러오는 방법 수정해야 함\n",
    "    MOV_to_MP4(mov_file_path)\n",
    "\n",
    "    # face\n",
    "    network = create_cnn_model()\n",
    "    image_to_be_matched_encoded = face_encodings()\n",
    "    cap, video_writer, video_fps, frame_cnt = Create_Video()\n",
    "    face_emotion = face_emotion_recognition(cap, network, image_to_be_matched_encoded, video_writer, frame_cnt, video_fps)\n",
    "    \n",
    "    # voice\n",
    "    output_file_path = \"output.wav\"  \n",
    "    convert_mov_to_wav(mov_file_path)\n",
    "    voice_emotion = voice_emotion_recognition()\n",
    "    # chatgpt_fine_tuning()\n",
    "    text = transcribe_audio(output_file_path)\n",
    "    stance, interpretation = analyze_fed_statement(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa033b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "print(face_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6055a19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neu']\n"
     ]
    }
   ],
   "source": [
    "print(voice_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98870f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Still dovish'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "870ee6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The speaker's comment reflects a cautious and hesitant approach towards rate hikes. They acknowledge the possibility of inflation persistence but emphasize the volatility and uncertainty in the current economic situation. This perspective implies a preference for delaying rate adjustments until a clearer and more stable understanding of the inflationary trends emerges.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e4b51e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def send_request(video_emotion, voice_emotion):\n",
    "    # Create a dictionary with face_emotion and voice_emotion\n",
    "    data = {\n",
    "        \"video_emotion\": video_emotion,\n",
    "        \"voice_emotion\": voice_emotion\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to JSON format\n",
    "    json_data = json.dumps(data)\n",
    "\n",
    "    # Specify the URL where you want to send the request\n",
    "    url = \"http://192.168.0.96:5001/emotion\"  # Replace with your actual API endpoint\n",
    "\n",
    "    # # Set headers if needed (e.g., content-type)\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    # Send the POST request\n",
    "    response = requests.post(url, data=json_data, headers=headers)\n",
    "\n",
    "    # Print the response\n",
    "    print(response)\n",
    "    return response;\n",
    "\n",
    "\n",
    "\n",
    "response = send_request(face_emotion, voice_emotion[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d758af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neu'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice_emotion[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3fc10e-3e4f-4173-b1f9-e4eb2e9bfa62",
   "metadata": {},
   "source": [
    "#### 화면 녹화 시작 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658b2ba2-ff53-4582-a3c7-1442f5a1b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_recording():\n",
    "    pyautogui.hotkey('command', 'shift', '5')\n",
    "    time.sleep(2)  \n",
    "    pyautogui.press('enter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce38400-1bac-42b0-91cd-77cdb5056e79",
   "metadata": {},
   "source": [
    "#### 화면 녹화 종료 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "639e2ab2-3904-47b1-a703-69fc6f7c458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_recording():\n",
    "    time.sleep(10)\n",
    "    pyautogui.hotkey('command', 'ctrl', 'esc')\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618b6b0-5d27-4be2-b30a-68bb244256a3",
   "metadata": {},
   "source": [
    "#### MOV to MP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a187c57-6e46-4519-a57b-eb6fddd63ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOV_to_MP4(mov_file_path):\n",
    "    mp4_file_path = 'powell video.mp4' # mp4파일 path\n",
    "\n",
    "    cap = cv2.VideoCapture(mov_file_path)\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You may need to adjust the codec based on your system\n",
    "    out = cv2.VideoWriter(mp4_file_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        out.write(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce51b94-1c7e-4164-a8fd-37d37af0be10",
   "metadata": {},
   "source": [
    "#### MOV to WAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16cd0f4a-112e-4787-96f1-833d6d78a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mov_to_wav(mov_file_path):\n",
    "    output_file_path = \"output.wav\"  # Replace with the desired output WAV file path\n",
    "\n",
    "    video_clip = VideoFileClip(mov_file_path)\n",
    "    audio_clip = video_clip.audio\n",
    "    audio_clip.write_audiofile(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7052f-13d7-446a-99b1-e0ae2f414fb6",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7da9492-cdc0-4c6e-bf1d-1b6b913c8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    num_classes = 7\n",
    "    num_detectors = 32\n",
    "    width, height = 48, 48\n",
    "    \n",
    "    network = Sequential()\n",
    "\n",
    "    network.add(Conv2D(filters=num_detectors, kernel_size=3, activation='relu', padding='same', input_shape=(width, height, 3)))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Conv2D(filters=num_detectors, kernel_size=3, activation='relu', padding='same'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    network.add(Dropout(0.2))\n",
    "\n",
    "    network.add(Conv2D(2*num_detectors, 3, activation='relu', padding='same'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Conv2D(2*num_detectors, 3, activation='relu', padding='same'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    network.add(Dropout(0.2))\n",
    "\n",
    "    network.add(Conv2D(2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Conv2D(2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    network.add(Dropout(0.2))\n",
    "\n",
    "    network.add(Conv2D(2*2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Conv2D(2*2*2*num_detectors, 3, activation='relu', padding='same'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    network.add(Dropout(0.2))\n",
    "\n",
    "    network.add(Flatten())\n",
    "\n",
    "    network.add(Dense(2*2*num_detectors, activation='relu'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Dropout(0.2))\n",
    "\n",
    "    network.add(Dense(2*num_detectors, activation='relu'))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Dropout(0.2))\n",
    "\n",
    "    network.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    with open('trained_network.json', 'w') as trained_network_json:\n",
    "        trained_network_json.write(network.to_json())\n",
    "    \n",
    "    with open('trained_network.json', 'r') as trained_network_json:\n",
    "        trained_model_json = trained_network_json.read()\n",
    "    \n",
    "    network = tf.keras.models.model_from_json(trained_model_json)\n",
    "    network.load_weights('weights_emotions.hdf5')\n",
    "    network.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccaa630-3c97-4dc9-8c59-b012f3274869",
   "metadata": {},
   "source": [
    "#### Powell Face Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb8c88c-db2f-4b22-b0b0-c2ed12d98f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_encodings():\n",
    "    image_to_be_matched = face_recognition.load_image_file('powell.png')\n",
    "    name = \"Chair Powell\"\n",
    "    \n",
    "    # encoded the loaded image into a feature vector\n",
    "    image_to_be_matched_encoded = face_recognition.face_encodings(image_to_be_matched)[0]\n",
    "    return image_to_be_matched_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f5503-690b-42fc-825b-b13c6cb4c943",
   "metadata": {},
   "source": [
    "#### Create Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e0f63c9-23e1-4508-bd92-caa25c7d117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Video():\n",
    "    cap = cv2.VideoCapture('powell video.mp4')\n",
    "    \n",
    "    video_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # Video capture's frame width\n",
    "    video_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # Video capture's frame height\n",
    "    video_size = (round(video_width), round(video_height)) # Video size\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)  # FPS(Frames Per Second)\n",
    "    frame_cnt = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Number of frames\n",
    "    \n",
    "    print('Number of frames:', frame_cnt, '/ FPS:', round(video_fps), '/ Frame size:', video_size)\n",
    "    \n",
    "    # In Linux, the extension of video output must be set to avi\n",
    "    video_output_path = 'emotion_classification_result2.avi'\n",
    "    \n",
    "    codec = cv2.VideoWriter_fourcc(*'XVID')  # Set the codec\n",
    "    \n",
    "    video_writer = cv2.VideoWriter(video_output_path, codec, video_fps, video_size)\n",
    "    return cap, video_writer, video_fps, frame_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e954b3a-29ff-4bef-8530-a22b70cd6d06",
   "metadata": {},
   "source": [
    "#### Face emotion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca285b0-327f-43ff-9880-626a0e8ecc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_emotion_recognition(cap, network, image_to_be_matched_encoded, video_writer, frame_cnt, video_fps):\n",
    "    data_list = []\n",
    "    green_color = (0, 255, 0)\n",
    "    red_color = (0, 0, 255)\n",
    "    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "    count = 1\n",
    "    skip_frames = 100  # Number of frames to skip\n",
    "\n",
    "    while cv2.waitKey(1) < 0:\n",
    "        connected, frame = cap.read()  # Read one frame from a VideoCapture object\n",
    "        # frame에서 얼굴 검출\n",
    "        if not connected:  # 영상 다 돌았을 때, break\n",
    "            break\n",
    "\n",
    "        # Skip frames if count is not a multiple of skip_frames\n",
    "        if count % skip_frames != 0:\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        face_detections = face_recognition.face_locations(frame, number_of_times_to_upsample=0, model=\"cnn\")  # CNN 기반 얼굴 검출기\n",
    "        # 검출된 얼굴 개수가 0보다 크면 수행\n",
    "        if len(face_detections) > 0:\n",
    "            for face_detection in face_detections:\n",
    "                # Print the location of each face in this image\n",
    "                top, right, bottom, left = face_detection\n",
    "                # 얼굴 위치\n",
    "                face_image = np.ascontiguousarray(frame[top:bottom, left:right])\n",
    "\n",
    "                # 얼굴 이미지 encoding\n",
    "                try:\n",
    "                    face_encoded = face_recognition.face_encodings(face_image)[0]\n",
    "                    result = face_recognition.compare_faces([image_to_be_matched_encoded], face_encoded, 0.5)\n",
    "                    # 연준의장이면 수행\n",
    "                    if result[0] == True:\n",
    "                        cv2.rectangle(frame, (left, top), (right, bottom), green_color, 2)\n",
    "                        roi = face_image\n",
    "                        # 감정분석\n",
    "                        roi = cv2.resize(roi, (48, 48))  # Extract region of interest from image\n",
    "                        roi = roi / 255  # Normalize\n",
    "                        roi = np.expand_dims(roi, axis=0)\n",
    "                        preds = network.predict(roi)\n",
    "                        print(\"Predictions:\", preds)\n",
    "                        # 라벨링 및 영상 frame부분에 감정값 렌더링\n",
    "                        if preds is not None:\n",
    "                            frame_time = count / video_fps\n",
    "                            emotion_probabilities = preds[0]  # 모든 감정 클래스의 확률 배열\n",
    "                            pred_emotion_index = np.argmax(emotion_probabilities)  # 가장 높은 확률을 가진 감정 클래스의 인덱스\n",
    "                            predicted_emotion = emotions[pred_emotion_index]  # 가장 높은 확률을 가진 감정 클래스\n",
    "                            data_list.append(\n",
    "                                {\"time\": frame_time, \"emotion\": predicted_emotion, **{emotions[i]: prob for i, prob in\n",
    "                                                                                    enumerate(emotion_probabilities)}}\n",
    "                            )\n",
    "                            print(\"Data List After Appending:\", data_list)\n",
    "                            cv2.putText(frame, emotions[pred_emotion_index], (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                        0.5, red_color, 1)\n",
    "                    # 연준의장이 아니면 넘김\n",
    "                    else:\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        video_writer.write(frame)\n",
    "\n",
    "        # Calculate and display the progress\n",
    "        progress = (count / frame_cnt) * 100\n",
    "        print(f\"Progress: {progress:.2f}%\")\n",
    "        count += 1\n",
    "\n",
    "        # if count >= 1000:\n",
    "        #     break\n",
    "\n",
    "    # Create the DataFrame outside the loop\n",
    "    df = pd.DataFrame(data_list)\n",
    "    # Calculate the average based on selected emotions\n",
    "    df[\"Average\"] = df[emotions].mean(axis=1)\n",
    "\n",
    "    main_emotion = df.loc[df[\"Average\"].idxmax(), \"emotion\"]\n",
    "    return main_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40490bc1-268d-4561-b28d-ede4609648d9",
   "metadata": {},
   "source": [
    "#### voice emotion recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc7b2c0-f7c8-45e2-b611-dc3f43ffdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_emotion_recognition():\n",
    "    classifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\")\n",
    "    \n",
    "    out_prob, score, index, text_lab = classifier.classify_file(\"output.wav\") #화자분리된 오디오 파일 위치\n",
    "    return text_lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e452f44-bfb8-4dc5-b31a-fe3086c4d3c6",
   "metadata": {},
   "source": [
    "#### Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e702d050-c89f-4c3d-94f6-71a0f54b833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(file_path):\n",
    "    url = \"https://transcribe.whisperapi.com\"\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer ZAZ7CT9CBPV9RSNWBPF25GSL9ZER6L8V'\n",
    "    }\n",
    "    \n",
    "    # Open the audio file in binary mode\n",
    "    file = {'file': open(file_path, 'rb')}\n",
    "    \n",
    "    data = {\n",
    "        \"fileType\": \"wav\",  # default is wav\n",
    "        \"diarization\": \"false\",\n",
    "        \"numSpeakers\": \"1\",\n",
    "        \"initialPrompt\": \"\",\n",
    "        \"language\": \"en\",\n",
    "        \"task\": \"transcribe\",\n",
    "        \"callbackURL\": \"\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, files=file, data=data)\n",
    "    json_response = response.json()\n",
    "    \n",
    "    # Extracted text from the response\n",
    "    extracted_text = json_response.get('text', '')\n",
    "    \n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78f591-cc3e-46c1-b67c-cc6ae4db2eea",
   "metadata": {},
   "source": [
    "#### Chatgpt Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0927c15d-daf2-4938-afad-83bb7742dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_fine_tuning():\n",
    "    jsonl_data = [\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"As a economist, your task is to analyze the FED's monetary policy stance at the FOMC press conference. I'll give you the paragraph that the Fed Chairman said. Then you have to do the three tasks below. First, understand the context of paragraphs in the statement provided. Second, contextually, each sentence should be categorized into one of five categories: dovish, neutral, mostly hawkish, or hawkish. Third, you need to explain why you categorized the statement into a specific category.\"},\n",
    "            {\"role\": \"assistant\", \"content\":\"Understood. I will analyze the statement as per the instructions.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"In light of the uncertainties and risks, and how far we have come, the Committee is proceeding carefully\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Stance: Mostly dovish , Interpretation: The phrase 'proceeding carefully' reflects a cautious approach in policy-making. This suggests a balance between maintaining a hawkish bias and acknowledging potential economic slowdowns, which might preclude further interest rate hikes. The emphasis is on careful decision-making considering current economic uncertainties and risks.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"We have asset purchases, and we’ve now said that we won’t go any lower than this, and—but—but that we’re prepared to adjust as appropriate, and rates are at the effective lower bound.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Stance: dovish , Interpretation: The reference to 'asset purchases' and 'we won’t go any lower than this' indicates a robust intention to maintain the current asset purchase levels. The term 'rates are at the effective lower bound' signifies that rates are already at a significantly low level, implying no further rate reductions are currently under consideration.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Economic activity remained resilient, job creation remains strong while cooling a bit, and the June CPI report actually came in a bit better than expectations for a change. And the June CPI report, of course, was welcomed, but it’s only one report, one month’s data. We hope that inflation will follow a lower path, as was—that will be consistent with the CPI reading, but we don’t know that, and we’re just going to need to see more data.\"}, \n",
    "            {\"role\": \"assistant\", \"content\":  \"Stance: Dovish , Interpretation: The acknowledgment of resilient economic activity and the better-than-expected June CPI report suggests an optimistic outlook, aligning with a dovish stance. Chairman Powell's comments during the press conference that the decision for additional rate hikes depends on further data and his favorable view of the June CPI report further support a dovish interpretation. This implies a cautious approach to policy tightening, waiting for more data to confirm the inflation trend.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Nominal wage growth has shown signs of easing, and job vacancies have declined so far this year. While the jobs-to-workers gap has declined, labor demand still substantially exceeds the supply of available workers.\"}, \n",
    "            {\"role\": \"assistant\", \"content\":  \"Stance: Mostly Dovish , Interpretation: This statement notes a slight easing in the labor market. Chairman Powell's remarks about the labor market showing signs of easing, suggest a slightly dovish stance compared to previous more assertive statements about the tightness of the labor market. This indicates a recognition of potential changes in labor market dynamics, which might influence future policy decisions.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"In light of the cumulative tightening of monetary policy and the lags with which monetary policy affects economic activity and inflation, the Committee decided to raise interest rates by 50 basis points today, a step-down from the 75 basis point pace seen over the previous four meetings.\"}, \n",
    "            {\"role\": \"assistant\", \"content\":  \"Stance: Dovish , Interpretation: The shift from a 75 basis point increase to a more moderate 50 basis point rise, despite ongoing high inflation risks, indicates a cautious approach and a potential pivot towards less aggressive monetary tightening. This moderation, especially considering Chairman Powell's earlier exclusion of a 75 basis point hike possibility, can be interpreted as a dovish signal amidst the prevailing inflation concerns. It suggests a balancing act between managing inflation and not over-tightening in a way that could significantly dampen economic activity.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"In determining the extent of additional policy firming that may be appropriate to return inflation to 2 percent over time, the Committee will take into account the cumulative tightening of monetary policy, the lags with which monetary policy affects economic activity and inflation, and economic and financial developments.\"}, \n",
    "            {\"role\": \"assistant\", \"content\":  \"Stance: Mostly hawkish , Interpretation: This sentence means continued openness to policy tightening to curb inflation, but a cautious approach that takes into account the cumulative impact of previous policies and economic development. This refers to most hawkish positions with a subtle perspective on future policy actions.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"In light of how far we’ve come in tightening policy, the uncertain lags with which monetary policy affects the economy, and potential headwinds from credit tightening, the Committee decided at today’s meeting to maintain the target range for the federal funds rate at 5 to 5¼ percent and to continue the process of significantly reducing our securities holdings.\"}, \n",
    "            {\"role\": \"assistant\", \"content\":  \"Stance: Hawkish , Interpretation: Despite maintaining the current federal funds rate, the emphasis on the progress in tightening policy and the continuation of reducing securities holdings indicates a hawkish stance. The decision reflects an acknowledgment of the impacts of previous policy decisions and a readiness to possibly increase rates in the future.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"In determining the extent of additional policy firming that may be appropriate to return inflation to 2 percent over time, the Committee will take into account the cumulative tightening of monetary policy, the lags with which monetary policy affects economic activity and inflation, and economic and financial developments.\"}, \n",
    "            {\"role\": \"assistant\", \"content\":  \"Stance: mostly hawkish , Interpretation: The mention of 'additional policy firming' strongly suggests a possibility of rate hikes. This underscores a careful consideration to bring inflation back sustainably to the 2 percent target.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"As shown in the SEP, the median projection for the appropriate level of the federal funds rate is 4.4 percent at the end of this year, 1 percentage point higher than projected in June.\"}, \n",
    "            {\"role\": \"assistant\", \"content\": \"Stance: Hawkish , Interpretation: The mention of 'additional policy firming' strongly suggests a possibility of rate hikes. This underscores a careful consideration to bring inflation back sustainably to the 2 percent target.\"}\n",
    "        ]},\n",
    "        {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"All FOMC participants forecast that this remaining test will be met next year. The median projection for the appropriate level of the federal funds rate is 0.9 percent at the end of 2022, about ½ percentage point higher than projected in September.\"}, \n",
    "            {\"role\": \"assistant\", \"content\":  \"Stance: Hawkish , Interpretation: The median projection for the federal funds rate being adjusted upwards by 0.5 percentage points compared to previous projections in September indicates a shift towards more aggressive monetary tightening. This significant upward adjustment in the rate forecast, along with the consensus on meeting future criteria, points to a proactive and assertive approach to managing inflation and economic growth.\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 파일에 JSON Lines 형식으로 저장\n",
    "    with open('make_jsonl.jsonl', 'w') as file:\n",
    "        for entry in jsonl_data:\n",
    "            json_line = json.dumps(entry)  # 딕셔너리를 JSON 문자열로 변환\n",
    "            file.write(json_line + '\\n')  # 파일에 JSON 문자열과 줄바꿈 추가\n",
    "\n",
    "    with open(\"make_jsonl.jsonl\") as f: \n",
    "        for line in f: print(line)\n",
    "\n",
    "    \n",
    "    openai.api_key=\"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\"\n",
    "    \n",
    "    training_response=openai.File.create(\n",
    "            file=open(\"make_jsonl.jsonl\",\"rb\"),\n",
    "            purpose='fine-tune'\n",
    "    )\n",
    "    \n",
    "    training_file_id = training_response[\"id\"]\n",
    "    \n",
    "    print (\"Training file id:\",training_file_id)\n",
    "\n",
    "    id=\"file-umorJCarC7TYCczQ8TAbkL4d\"\n",
    "    \n",
    "    response = openai.FineTuningJob.create(\n",
    "    training_file=id,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    \n",
    "    job_id=response[\"id\"]\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af0161e6-c6ca-4bf0-a5c1-23b352480219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석 함수 정의\n",
    "def analyze_fed_statement(st1):\n",
    "    openai.api_key=\"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\"\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"ft:gpt-3.5-turbo-0613:personal::8NkY7Btw\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"As an economist, your task is to analyze the FED's monetary policy stance at the FOMC press conference based on the given paragraph. Consider the overall context and tone of the paragraph to categorize it into one of the following categories: dovish, neutral, mostly hawkish, or hawkish. Then, pexplain why you categorized the statement into a specific category for your categorization, focusing on the paragraph as a whole rather than analyzing each sentence separately. In other words, you can say 'stance' and 'interpretation'. Here's the text I'm providing.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Analyze this statement: {st1}\"}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=250,\n",
    "    )\n",
    "    result = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    stance_start = result.find('Stance:')\n",
    "    interpretation_start = result.find('Interpretation:')\n",
    "    \n",
    "    # Extract Stance\n",
    "    if stance_start != -1:\n",
    "        stance_end = result.find(',', stance_start)\n",
    "        stance = result[stance_start + len('Stance:'):stance_end].strip()\n",
    "    else:\n",
    "        stance = None\n",
    "    \n",
    "    # Extract Interpretation\n",
    "    if interpretation_start != -1:\n",
    "        interpretation = result[interpretation_start + len('Interpretation:'):].strip()\n",
    "    else:\n",
    "        interpretation = None\n",
    "\n",
    "    return stance, interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7021ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
